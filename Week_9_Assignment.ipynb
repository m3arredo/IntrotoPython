{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6y1-rf_IL7Y"
   },
   "source": [
    "# Natural Language Processing with `nltk`\n",
    "\n",
    "`nltk` is the most popular Python package for Natural Language processing, it provides algorithms for importing, cleaning, pre-processing text data in human language and then apply computational linguistics algorithms like sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHjb2Y_3IL7d"
   },
   "source": [
    "## Inspect the Movie Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z92_bzFXIL7d"
   },
   "source": [
    "It also includes many easy-to-use datasets in the `nltk.corpus` package, we can download for example the `movie_reviews` package using the `nltk.download` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XGa-UI_h2N6t"
   },
   "outputs": [],
   "source": [
    "# Uncomment the below line and run this cell if you need to install nltk\n",
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y2MO0CZQIL7e"
   },
   "outputs": [],
   "source": [
    "#Run this cell for all the imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1-y5EE-IL7g",
    "outputId": "315e63ce-227e-44e0-938e-9bfb8412d61f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\mary0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell to download the dataset\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp-b5EB8IL7h"
   },
   "source": [
    "You can also list and download other datasets interactively just typing:\n",
    "\n",
    "`nltk.download()`\n",
    "    \n",
    "in the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R0ipHkMJIL7i"
   },
   "outputs": [],
   "source": [
    "#Run this cell to import the dataset\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6bt0M-Yt2DoW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\mary0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mary0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell for later use in tokenization\n",
    "nltk.download('vader_lexicon')  # for sentiment analysis\n",
    "nltk.download('punkt')  # for tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiyDcXM0IL7o"
   },
   "source": [
    "## 1. Tokenize Text in Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JSCLvHkAIL7p"
   },
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "romeo_text = \"\"\"Why then, O brawling love! O loving hate!\n",
    "O any thing, of nothing first create!\n",
    "O heavy lightness, serious vanity,\n",
    "Misshapen chaos of well-seeming forms,\n",
    "Feather of lead, bright smoke, cold fire, sick health,\n",
    "Still-waking sleep, that is not what it is!\n",
    "This love feel I, that feel no love in this.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pmnjVuoIL7p"
   },
   "source": [
    "The first step in Natural Language processing is generally to split the text into words, this process might appear simple but it is very tedious to handle all corner cases, see for example all the issues with punctuation we have to solve if we just start with a split on whitespace.\n",
    "\n",
    "1.1 **Split `romeo_text` by spaces and print the resultant list of words** [0.5 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QC7jXTAZIL7q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why',\n",
       " 'then,',\n",
       " 'O',\n",
       " 'brawling',\n",
       " 'love!',\n",
       " 'O',\n",
       " 'loving',\n",
       " 'hate!',\n",
       " 'O',\n",
       " 'any',\n",
       " 'thing,',\n",
       " 'of',\n",
       " 'nothing',\n",
       " 'first',\n",
       " 'create!',\n",
       " 'O',\n",
       " 'heavy',\n",
       " 'lightness,',\n",
       " 'serious',\n",
       " 'vanity,',\n",
       " 'Misshapen',\n",
       " 'chaos',\n",
       " 'of',\n",
       " 'well-seeming',\n",
       " 'forms,',\n",
       " 'Feather',\n",
       " 'of',\n",
       " 'lead,',\n",
       " 'bright',\n",
       " 'smoke,',\n",
       " 'cold',\n",
       " 'fire,',\n",
       " 'sick',\n",
       " 'health,',\n",
       " 'Still-waking',\n",
       " 'sleep,',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'what',\n",
       " 'it',\n",
       " 'is!',\n",
       " 'This',\n",
       " 'love',\n",
       " 'feel',\n",
       " 'I,',\n",
       " 'that',\n",
       " 'feel',\n",
       " 'no',\n",
       " 'love',\n",
       " 'in',\n",
       " 'this.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romeo_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibunUjDBIL7q"
   },
   "source": [
    "`nltk` has a sophisticated word tokenizer trained on English named `punkt` which we imported earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fp7qOeSIL7r"
   },
   "source": [
    "1.2  **Use the `nltk.word_tokenize(text)` function to properly tokenize `romeo_text` and stores the result as `romeo_words`. Print the resultant list.** Compare it to the whitespace splitting we used above and mention the difference. [0.5 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "k-Q7oXh9IL7r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why', 'then', ',', 'O', 'brawling', 'love', '!', 'O', 'loving', 'hate', '!', 'O', 'any', 'thing', ',', 'of', 'nothing', 'first', 'create', '!', 'O', 'heavy', 'lightness', ',', 'serious', 'vanity', ',', 'Misshapen', 'chaos', 'of', 'well-seeming', 'forms', ',', 'Feather', 'of', 'lead', ',', 'bright', 'smoke', ',', 'cold', 'fire', ',', 'sick', 'health', ',', 'Still-waking', 'sleep', ',', 'that', 'is', 'not', 'what', 'it', 'is', '!', 'This', 'love', 'feel', 'I', ',', 'that', 'feel', 'no', 'love', 'in', 'this', '.']\n"
     ]
    }
   ],
   "source": [
    "romeo_words = nltk.word_tokenize(romeo_text)\n",
    "print(romeo_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that the string from the nltk is separated by punctuations such as exclamation marks and commas, while the split method is only separated by whitespaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc1x10nEIL7t"
   },
   "source": [
    "## 2. Build a bag-of-words model\n",
    "\n",
    "The simplest model for analyzing text is just to think about text as an unordered collection of words (bag-of-words). This can generally allow to infer from the text the category, the topic or the sentiment.\n",
    "\n",
    "From the bag-of-words model we can build features to be used by a classifier, here we assume that each word is a feature that can either be `True` or `False`.\n",
    "We implement this in Python as a dictionary where for each word in a sentence we associate `True`.\n",
    "\n",
    "2.1 **Write a function `build_bag_of_words(words)` that returns such a dictionary with {word : True} format given a set of words. Call the function with `romeo_words` and print the resultant dictionary.** [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YNopS_EkIL7u"
   },
   "outputs": [],
   "source": [
    "def build_bag_of_words(words):\n",
    "    x = {words : True for words in romeo_words}\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Why': True,\n",
       " 'then': True,\n",
       " ',': True,\n",
       " 'O': True,\n",
       " 'brawling': True,\n",
       " 'love': True,\n",
       " '!': True,\n",
       " 'loving': True,\n",
       " 'hate': True,\n",
       " 'any': True,\n",
       " 'thing': True,\n",
       " 'of': True,\n",
       " 'nothing': True,\n",
       " 'first': True,\n",
       " 'create': True,\n",
       " 'heavy': True,\n",
       " 'lightness': True,\n",
       " 'serious': True,\n",
       " 'vanity': True,\n",
       " 'Misshapen': True,\n",
       " 'chaos': True,\n",
       " 'well-seeming': True,\n",
       " 'forms': True,\n",
       " 'Feather': True,\n",
       " 'lead': True,\n",
       " 'bright': True,\n",
       " 'smoke': True,\n",
       " 'cold': True,\n",
       " 'fire': True,\n",
       " 'sick': True,\n",
       " 'health': True,\n",
       " 'Still-waking': True,\n",
       " 'sleep': True,\n",
       " 'that': True,\n",
       " 'is': True,\n",
       " 'not': True,\n",
       " 'what': True,\n",
       " 'it': True,\n",
       " 'This': True,\n",
       " 'feel': True,\n",
       " 'I': True,\n",
       " 'no': True,\n",
       " 'in': True,\n",
       " 'this': True,\n",
       " '.': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_bag_of_words(romeo_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2Gq7K-PIL7u"
   },
   "source": [
    "This is what we wanted, but we notice that also punctuation like \"!\" and words useless for classification purposes like \"of\" or \"that\" are also included.\n",
    "Those words are named \"stopwords\" and `nltk` has a convenient corpus we can download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lhg432k0IL7v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mary0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "B2ZSkAo3IL7v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k9-f6hsIL7v"
   },
   "source": [
    "Using the Python `string.punctuation` list and the English stopwords we can build better features by filtering out those words that would not help in the classification. \n",
    "\n",
    "2.2 **Create a list `useless_words` that is a collection of stopwords in english and the punctuation characters.** [0.5 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Uves5SsoIL7w"
   },
   "outputs": [],
   "source": [
    "useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n",
    "#print(useless_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcyRfXlZ4x-H"
   },
   "source": [
    "2.3 **Write a function `build_bag_of_words_features_filtered(words)` that returns a filtered bag of words - a dictionary with only useful words as key and 1 as the value. Call this function with `romeo_words` and print the resultant list.** [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oYoBFYAkIL7w"
   },
   "outputs": [],
   "source": [
    "def build_bag_of_words_features_filtered(words): \n",
    "    x = { words : 1 for words in romeo_words \\\n",
    "         if not words in useless_words} # if not, they're ignored\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Why': 1,\n",
       " 'O': 1,\n",
       " 'brawling': 1,\n",
       " 'love': 1,\n",
       " 'loving': 1,\n",
       " 'hate': 1,\n",
       " 'thing': 1,\n",
       " 'nothing': 1,\n",
       " 'first': 1,\n",
       " 'create': 1,\n",
       " 'heavy': 1,\n",
       " 'lightness': 1,\n",
       " 'serious': 1,\n",
       " 'vanity': 1,\n",
       " 'Misshapen': 1,\n",
       " 'chaos': 1,\n",
       " 'well-seeming': 1,\n",
       " 'forms': 1,\n",
       " 'Feather': 1,\n",
       " 'lead': 1,\n",
       " 'bright': 1,\n",
       " 'smoke': 1,\n",
       " 'cold': 1,\n",
       " 'fire': 1,\n",
       " 'sick': 1,\n",
       " 'health': 1,\n",
       " 'Still-waking': 1,\n",
       " 'sleep': 1,\n",
       " 'This': 1,\n",
       " 'feel': 1,\n",
       " 'I': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_bag_of_words_features_filtered(romeo_text) # most of the words like is and punctuation are gone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uEVZeRtIL7x"
   },
   "source": [
    "## 3. Frequencies of Words\n",
    "\n",
    "It is common to explore a dataset before starting the analysis, in this section we will find the most common words and plot their frequency.\n",
    "\n",
    "3.1. Using the `movie_reviews.words()` (the nltk corpus we imported previously) with no argument we can extract the words from the entire dataset as `all_words` and check that it is about 1.6 millions. [0.5 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aWARu4cyIL7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583820\n"
     ]
    }
   ],
   "source": [
    "all_words = movie_reviews.words()\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avr7Yxg5IL7x"
   },
   "source": [
    "3.2. Filter out `useless_words` as defined in the previous section, and create a new list `filtered_words` this will reduce the length of the dataset by more than a factor of 2. (Hint - python list comprehension) [0.5 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lHXhpCVIIL7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710579\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [w for w in all_words if not w in useless_words ]\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Hlk-3IIL7y"
   },
   "source": [
    "The `collection` package of the standard library contains a `Counter` class that is handy for counting frequencies of words in our list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "To1JVHR2IL7y"
   },
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "from collections import Counter\n",
    "word_counter = Counter(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SP7YAIVIL7y"
   },
   "source": [
    "Use the [most_common() ](https://pythontic.com/containers/counter/most_common) method of the word_counter and print the top 10 used words from the corpus as a list called `most_common_words`. [0.5 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "TOWHdLsKIL7y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 9517), ('one', 5852), ('movie', 5771), ('like', 3690), ('even', 2565), ('good', 2411), ('time', 2411), ('story', 2169), ('would', 2109), ('much', 2049)]\n"
     ]
    }
   ],
   "source": [
    "most_common_words = word_counter.most_common(10)\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEQdW9yT-iBh"
   },
   "source": [
    "## 4. Sentiment Analysis [2 pt]\n",
    "\n",
    "Using the sentiment intensity analyzer, loop over the `list_sentences` and print the polarity scores of each of the sentence. (Hint - refer to lecture notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "RUGwhY5ylAeQ"
   },
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "list_sentences = [\"Hello, how are you?\", \"Today is a nice day\", \"I don't like the food at the cafe\", \"This is the worst pizza I have ever had.\", \"The orange juice is delicious!\", \"I am late to class.\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "_Uv4u1Tbk1eT"
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.517, 'pos': 0.483, 'compound': 0.4215}\n",
      "{'neg': 0.26, 'neu': 0.74, 'pos': 0.0, 'compound': -0.2755}\n",
      "{'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
      "{'neg': 0.0, 'neu': 0.501, 'pos': 0.499, 'compound': 0.6114}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_sentences)):\n",
    "    scores = sia.polarity_scores(list_sentences[i])    \n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO4rFOfJphTG"
   },
   "source": [
    "# 5. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lC-Ne7QjpkGx"
   },
   "source": [
    "### 5.1 Experiment [2.5 pt]\n",
    "\n",
    "For this section, we will use the same notebook from the lecture videos and perform tests on it. We learned that neural networks consist of layers of interconnected neurons/nodes that make up the hidden layers with weights that work on incoming information using the activation function. \n",
    "\n",
    "Make a copy and run (run all) this [notebook](https://drive.google.com/file/d/1JtkYzdEHl1ijLvralyla_2bNgaPODCM0/view?usp=sharing). USE COLAB ONLY.\n",
    "\n",
    "Perform the following experiments and report the test accuracy and time taken (sum of time taken across 10 epochs while fitting). \n",
    "\n",
    "1. For example - Provided case<br>\n",
    "1st hidden layer - 128 nodes.\n",
    "Accuracy : 0.9781\n",
    "Time: 43s\n",
    "\n",
    "For the subsequent experiments, you need to make a change in the cell [15] like this - <br>\n",
    "  ```model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'), #given hidden layer 1\n",
    "      tf.keras.layers.Dense(128, activation='relu'), #added this layer for Experiment 2\n",
    "      tf.keras.layers.Dense(10)])\n",
    "  ```\n",
    "\n",
    "2. Experiment 2 <br>\n",
    "1st hidden layer - 128 nodes.<br>\n",
    "2nd hidden layer - 128 nodes.<br>\n",
    "\n",
    "3. Experiment 3 <br>\n",
    "1st hidden layer - 256 nodes.<br>\n",
    "\n",
    "\n",
    "3. Experiment 4 <br>\n",
    "1st hidden layer - 256 nodes.<br>\n",
    "2nd hidden layer - 256 nodes.<br>\n",
    "\n",
    "\n",
    "4. Experiment 5 <br>\n",
    "1st hidden layer - 128 nodes.<br>\n",
    "2nd hidden layer - 128 nodes.<br>\n",
    "3rd hidden layer - 128 nodes.<br>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-g35cOzmBYGM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Experiment 1:\n",
    "# Accuracy:      Time:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ni8gQwTavltw"
   },
   "source": [
    "### 5.2 Inference [0.5 pt]\n",
    "What can you infer from the above experiments regarding accuracy and computation resources(time) by changing the number of layers/number of nodes each layer. For any of the experiment did the accuracy decrease unexpectedly - what could be the reason? [0.5 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjS8X4zppjZ9"
   },
   "outputs": [],
   "source": [
    "#ToDo"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
